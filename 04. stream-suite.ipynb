{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13733e9f-5ad8-452d-bf75-ffbe49bcca20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./03_stream_invoice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a432b3cd-8275-4942-b474-0d52d7d0e0b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Cleanup...Done\nStarting Invoice Processing Stream...Done\n\nTesting first iteration of invoice stream...\n\tStarting Ingestion...Done\n\tWaiting for 30 seconds...Done.\n\tStarting validation...Done\nValidation passed.\n\nTesting second iteration of invoice stream...\n\tStarting Ingestion...Done\n\tWaiting for 30 seconds...Done.\n\tStarting validation...Done\nValidation passed.\n\nTesting third iteration of invoice stream...\n\tStarting Ingestion...Done\n\tWaiting for 30 seconds...Done.\n\tStarting validation...Done\nValidation passed.\n\n"
     ]
    }
   ],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %run ./03-invoice-stream\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "class invoiceStreamTestSuite():\n",
    "    def __init__(self):\n",
    "        self.base_data_dir = \"/FileStore/data_spark_streaming_scholarnest\"\n",
    "\n",
    "    def cleanTests(self):\n",
    "        print(f\"Starting Cleanup...\", end='')\n",
    "        spark.sql(\"drop table if exists invoice_line_items\")\n",
    "        dbutils.fs.rm(\"/user/hive/warehouse/invoice_line_items\", True)\n",
    "\n",
    "        dbutils.fs.rm(f\"{self.base_data_dir}/chekpoint/invoices\", True)\n",
    "        dbutils.fs.rm(f\"{self.base_data_dir}/data/invoices\", True)\n",
    "\n",
    "        dbutils.fs.mkdirs(f\"{self.base_data_dir}/data/invoices\")\n",
    "        print(\"Done\")\n",
    "\n",
    "    def ingestData(self, itr):\n",
    "        print(f\"\\tStarting Ingestion...\", end='')\n",
    "        dbutils.fs.cp(f\"{self.base_data_dir}/datasets/invoices/invoices_{itr}.json\", f\"{self.base_data_dir}/data/invoices/\")\n",
    "        print(\"Done\")\n",
    "\n",
    "    def assertResult(self, expected_count):\n",
    "        print(f\"\\tStarting validation...\", end='')\n",
    "        actual_count = spark.sql(\"select count(*) from invoice_line_items\").collect()[0][0]\n",
    "        assert expected_count == actual_count, f\"Test failed! actual count is {actual_count}\"\n",
    "        print(\"Done\")\n",
    "\n",
    "    def waitForMicroBatch(self, sleep=30):\n",
    "        import time\n",
    "        print(f\"\\tWaiting for {sleep} seconds...\", end='')\n",
    "        time.sleep(sleep)\n",
    "        print(\"Done.\")\n",
    "\n",
    "    def runTests(self):\n",
    "        self.cleanTests()\n",
    "        iStream = invoiceStream()\n",
    "        streamQuery = iStream.process()\n",
    "\n",
    "        print(\"Testing first iteration of invoice stream...\") \n",
    "        self.ingestData(1)\n",
    "        self.waitForMicroBatch()        \n",
    "        self.assertResult(1253)\n",
    "        print(\"Validation passed.\\n\")\n",
    "\n",
    "        print(\"Testing second iteration of invoice stream...\") \n",
    "        self.ingestData(2)\n",
    "        self.waitForMicroBatch()\n",
    "        self.assertResult(2510)\n",
    "        print(\"Validation passed.\\n\") \n",
    "\n",
    "        print(\"Testing third iteration of invoice stream...\") \n",
    "        self.ingestData(3)\n",
    "        self.waitForMicroBatch()\n",
    "        self.assertResult(3994)\n",
    "        print(\"Validation passed.\\n\")\n",
    "\n",
    "        streamQuery.stop()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "isTS = invoiceStreamTestSuite()\n",
    "isTS.runTests()\t\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC &copy; 2021-2023 <a href=\"https://www.scholarnest.com/\">ScholarNest Technologies Pvt. Ltd. </a>All rights reserved.<br/>\n",
    "# MAGIC <br/>\n",
    "# MAGIC <a href=\"https://www.scholarnest.com/privacy/\">Privacy Policy</a> | <a href=\"https://www.scholarnest.com/terms/\">Terms of Use</a> | <a href=\"https://www.scholarnest.com/contact-us/\">Contact Us</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f157fb3-0817-48e2-b62e-8b899e5dccf9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04. stream-suite",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
