{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9989f4ea-e3c7-4393-b3f8-b6e68e003b6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./05_streaming_trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da489601-2724-4bdf-afe1-9cf840856a14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Cleanup...Done\nStarting Invoice Processing Stream...Done\n\nTesting first iteration of invoice stream...\n\tStarting Ingestion...Done\n\tWaiting for 30 seconds...Done.\n\tStarting validation...Done\nValidation passed.\n\nTesting second iteration of invoice stream...\n\tStarting Ingestion...Done\n\tWaiting for 30 seconds...Done.\n\tStarting validation...Done\nValidation passed.\n\nTesting third iteration of invoice stream...\n\tStarting Ingestion...Done\n\tWaiting for 30 seconds...Done.\n\tStarting validation...Done\nValidation passed.\n\nStarting Cleanup...Done\nTesting first batch of invoice stream...\n\tStarting Ingestion...Done\n\tStarting Ingestion...Done\nStarting Invoice Processing Stream...Done\n\n\tWaiting for 30 seconds...Done.\n\tStarting validation...Done\nValidation passed.\n\nTesting second batch of invoice stream...\n\tStarting Ingestion...Done\nStarting Invoice Processing Stream...Done\n\n\tWaiting for 30 seconds...Done.\n\tStarting validation...Done\nValidation passed.\n\n"
     ]
    }
   ],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %run ./05-streaming-batch\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "class streamingBatchTestSuite():\n",
    "    def __init__(self):\n",
    "        self.base_data_dir = \"/FileStore/data_spark_streaming_scholarnest\"\n",
    "\n",
    "    def cleanTests(self):\n",
    "        print(f\"Starting Cleanup...\", end='')\n",
    "        spark.sql(\"drop table if exists invoice_line_items\")\n",
    "        dbutils.fs.rm(\"/user/hive/warehouse/invoice_line_items\", True)\n",
    "\n",
    "        dbutils.fs.rm(f\"{self.base_data_dir}/chekpoint/invoices\", True)\n",
    "        dbutils.fs.rm(f\"{self.base_data_dir}/data/invoices\", True)\n",
    "\n",
    "        dbutils.fs.mkdirs(f\"{self.base_data_dir}/data/invoices\")\n",
    "        print(\"Done\")\n",
    "\n",
    "    def ingestData(self, itr):\n",
    "        print(f\"\\tStarting Ingestion...\", end='')\n",
    "        dbutils.fs.cp(f\"{self.base_data_dir}/datasets/invoices/invoices_{itr}.json\", f\"{self.base_data_dir}/data/invoices/\")\n",
    "        print(\"Done\")\n",
    "\n",
    "    def assertResult(self, expected_count):\n",
    "        print(f\"\\tStarting validation...\", end='')\n",
    "        actual_count = spark.sql(\"select count(*) from invoice_line_items\").collect()[0][0]\n",
    "        assert expected_count == actual_count, f\"Test failed! actual count is {actual_count}\"\n",
    "        print(\"Done\")\n",
    "\n",
    "    def waitForMicroBatch(self, sleep=30):\n",
    "        import time\n",
    "        print(f\"\\tWaiting for {sleep} seconds...\", end='')\n",
    "        time.sleep(sleep)\n",
    "        print(\"Done.\")\n",
    "\n",
    "    def runStreamTests(self):\n",
    "        self.cleanTests()\n",
    "        iStream = invoiceStreamBatch()\n",
    "        streamQuery = iStream.process(\"30 seconds\")\n",
    "\n",
    "        print(\"Testing first iteration of invoice stream...\") \n",
    "        self.ingestData(1)\n",
    "        self.waitForMicroBatch()        \n",
    "        self.assertResult(1253)\n",
    "        print(\"Validation passed.\\n\")\n",
    "\n",
    "        print(\"Testing second iteration of invoice stream...\") \n",
    "        self.ingestData(2)\n",
    "        self.waitForMicroBatch()\n",
    "        self.assertResult(2510)\n",
    "        print(\"Validation passed.\\n\") \n",
    "\n",
    "        print(\"Testing third iteration of invoice stream...\") \n",
    "        self.ingestData(3)\n",
    "        self.waitForMicroBatch()\n",
    "        self.assertResult(3994)\n",
    "        print(\"Validation passed.\\n\")\n",
    "\n",
    "        streamQuery.stop()\n",
    "\n",
    "    def runBatchTests(self):\n",
    "        self.cleanTests()\n",
    "        iStream = invoiceStreamBatch()\n",
    "\n",
    "        print(\"Testing first batch of invoice stream...\") \n",
    "        self.ingestData(1)\n",
    "        self.ingestData(2)\n",
    "        iStream.process(\"batch\")\n",
    "        self.waitForMicroBatch(30)        \n",
    "        self.assertResult(2510)\n",
    "        print(\"Validation passed.\\n\")\n",
    "\n",
    "        print(\"Testing second batch of invoice stream...\") \n",
    "        self.ingestData(3)\n",
    "        iStream.process(\"batch\")\n",
    "        self.waitForMicroBatch(30)        \n",
    "        self.assertResult(3994)\n",
    "        print(\"Validation passed.\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "sbTS = streamingBatchTestSuite()\n",
    "sbTS.runStreamTests()\t\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "sbTS.runBatchTests()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC &copy; 2021-2023 <a href=\"https://www.scholarnest.com/\">ScholarNest Technologies Pvt. Ltd. </a>All rights reserved.<br/>\n",
    "# MAGIC Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "# MAGIC Databricks, Databricks Cloud and the Databricks logo are trademarks of the <a href=\"https://www.databricks.com/\">Databricks Inc</a>.<br/>\n",
    "# MAGIC <br/>\n",
    "# MAGIC <a href=\"https://www.scholarnest.com/privacy/\">Privacy Policy</a> | <a href=\"https://www.scholarnest.com/terms/\">Terms of Use</a> | <a href=\"https://www.scholarnest.com/contact-us/\">Contact Us</a>\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "06_stream_trigger_suite",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
